# Zero-shot Multilingual Sentiment Classification using mBERT

This is a simple project exploring how multilingual BERT (mBERT) performs in zero-shot sentiment classification.  
The model is trained using only one language (e.g. English), and evaluated on other languages (e.g. French, Chinese) to test its cross-lingual generalization ability.

## Goals

- Use mBERT to perform sentiment classification
- Train only on one language and evaluate on others (zero-shot setting)
- Compare the effect of different training languages on transfer accuracy

## Structure

- `notebooks/`: Jupyter notebooks for training and testing
- `data/`: Sample sentiment datasets (not included in this repo)
- `requirements.txt`: Python dependencies

## Status

ðŸš§ Work in progress.
